
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Q-Transformer</title>

    <meta name="description" content="Q-Transformer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://q-transformer.github.io/img/qt_title_overview.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://q-transformer.github.io/"/>
    <meta property="og:title" content="Q-Transformer" />
    <meta property="og:description" content="Project page for Q-Transformer." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Q-Transformer" />
    <meta name="twitter:description" content="Project page for Q-Transformer." />
    <meta name="twitter:image" content="https://q-transformer.github.io/img/qt_title_overview.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Q-Transformer: </font></strong> </br> Scalable Offline Reinforcement Learning via Autoregressive Q-Functions </br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Yevgen Chebotar<sup>*</sup></li> <li>Quan Vuong<sup>*</sup></li> <li>Karol Hausman</li> <li>Fei Xia</li> <li>Yao Lu</li> <li>Alex Irpan</li> <li>Aviral Kumar</li> <br>
                 <li>Tianhe Yu</li> <li>Alexander Herzog</li> <li>Karl Pertsch</li> <li>Keerthana Gopalakrishnan</li> <li>Julian Ibarz</li> <li>Ofir Nachum</li>  <br>
                  <li>Grecia Salazarn</li> <li>Huong T Tran</li> <li>Jodilyn Peralta</li> <li>Clayton Tan</li> <li>Deeksha Manjunath</li> <li>Jaspiar Singht</li> <br>
                  <li>Brianna Zitkovich</li> <li>Tomas Jackson</li>  <li>Kanishka Rao</li> <li>Chelsea Finn</li> <li>Sergey Levine</li><br>
                <br>
		   <i> <sup>*</sup>Equal contribution</i>
		<br><br>
                    <a href="http://g.co/robotics">
                    <img src="img/deepmind.png" height="67px"> </a>
                    
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="assets/qtransformer.pdf">
                            <image src="img/paper_small2.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://youtu.be/UuKAp9a6wMs">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
        	    <image src="img/qt_title_overview.png" class="img-responsive">
                </p>
		<p style="text-align:center;">
        	    <image src="img/qt_robot_frames.png" class="img-responsive">
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
			In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that are crucial to obtain good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large  diverse real-world robotic manipulation task suite.
                </p>
            </div>
        </div>

	<!--
	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/UuKAp9a6wMs" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>
	-->

<div class="row">
   <div class="col-md-8 col-md-offset-2">
	<br>
	<h3>
	    Approach
	</h3>
	<p class="text-justify">
	We first describe how to enable using Transformers for Q-learning by applying discretization and autoregression of the action space.
	The classical way for learning a Q-function using TD-learning is based on the Bellman update rule:
	<p style="text-align:center;">
	        <image src="img/bellman_original.png" class="img-responsive">
	</p>
	We change the Bellman update to be performed for each action dimension by transforming the original MDP of the problem into an MDP where each
	action dimension is treated as a separate step for Q-learning. In particular, given the action dimensionality <i>d<sub>A</sub></i>, the new Bellman update rule is:
	<p style="text-align:center;">
	        <image src="img/bellman_qt.png" class="img-responsive">
	</p>
	This means that for each intermediate action dimension we maximize over the next action dimension given the same state, 
	and for the final action dimension we use the first action dimension from the next state.

	In order to account for the distribution shift during offline learning, we introduce a simple regularization technique 
	that minimizes unseen actions (in the discretized case unseen action bins) to the lowest value. To accelerate learning, we also employ
	Monte-Carlo (MC) returns that use the original return-to-go from a given episode. The illustration of our full learning update
	can be seen below.
	<p style="text-align:center;">
	        <image src="img/qt_update_fig.png" class="img-responsive">
	</p>
	   
	Our network architecture is based on RT-1 with a modification to output Q-values instead of action probabilities.
	<p style="text-align:center;">
	        <image src="img/qt_network_arch.png" class="img-responsive">
	</p>		    
 </div>
</div>
	    
<div class="row">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Results
	</h3>
	<p class="text-justify">
	We evaluate ...	 <br> <br>
	We start with real world experiments...
	<p style="text-align:center;">
	    <image src="img/real_results.png" class="img-responsive">        	   
	</p>
	As can be seen...
	</p>

	<p class="text-justify">
		We also benchmark in simulation...
	</p>
	<p style="text-align:center;">
	    <image src="img/sim_exps.png"  class="img-responsive" height="600px">
	</p>
	
	<p class="text-justify">
	Next, we ablate our design choices...
	</p>
		
	  <p style="text-align:center;">
	    <image src="img/qt_ablations.png"  class="img-responsive" height="450px">
	</p>
		  
	<p> 
	 We also ablate n-step return...
	</p>
	 <p style="text-align:center;">
	    <image src="img/nstep_table.png"  class="img-responsive" height="600px">
	</p>
		  
	<p class="text-justify">
	To try to scale up the training...    
	</p>
		
	 <p style="text-align:center;">
	    <image src="img/large_offline_results.png"  class="img-responsive" height="600px">
	</p>

	<p class="text-justify">
	Finally, we employ Q-function as an affordance model...
	</p>
	 <p style="text-align:center;">
	    <image src="img/affordance_perf.png"  class="img-responsive" height="600px">
	</p>

	<p class="text-justify">
	We also combine it with a large language model...
	</p>
	 <p style="text-align:center;">
	    <image src="img/affordance_perf.png"  class="img-responsive" height="600px">
	</p>
	<p><
        Below we show two examples...
        </p>
	 <p style="text-align:center;">
	    <image src="img/saycan_qt_1.png"  class="img-responsive" height="600px">
	</p>
	 <p style="text-align:center;">
	    <image src="img/saycan_qt_1.png"  class="img-responsive" height="600px">
	</p>
	

    </div>
</div>
<div class="row">
    <div class="col-md-8 col-md-offset-2">
	<h3>
	    Video
	</h3>
	<video id="v0" width="100%" playsinline muted loop controls>
	       <source src="img/saycan_rt1_demo3_comp.mp4" type="video/mp4">
        </video>		
    </div>
</div>

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
		@inproceedings{rt12022arxiv,
		    title={RT-1: Robotics Transformer for Real-World Control at Scale},
		    author={Anthony	Brohan and  Noah Brown and  Justice Carbajal and  Yevgen Chebotar and  Joseph Dabis and  Chelsea Finn and  Keerthana Gopalakrishnan and  Karol Hausman and  Alex Herzog and  Jasmine Hsu and  Julian Ibarz and  Brian Ichter and  Alex Irpan and  Tomas Jackson and  Sally Jesmonth and  Nikhil Joshi and  Ryan Julian and  Dmitry Kalashnikov and  Yuheng Kuang and  Isabel Leal and  Kuang-Huei Lee and  Sergey Levine and  Yao Lu and  Utsav Malla and  Deeksha Manjunath and  Igor Mordatch and  Ofir Nachum and  Carolina Parada and  Jodilyn Peralta and  Emily Perez and  Karl Pertsch and  Jornell Quiambao and  Kanishka Rao and  Michael Ryoo and  Grecia Salazar and  Pannag Sanketi and  Kevin Sayed and  Jaspiar Singh and  Sumedh Sontakke and  Austin Stone and  Clayton Tan and  Huong Tran and  Vincent Vanhoucke and Steve Vega and  Quan Vuong and  Fei Xia and  Ted Xiao and  Peng Xu and  Sichun Xu and  Tianhe Yu and  Brianna Zitkovich},
		    booktitle={arXiv preprint arXiv:2212.06817},
		    year={2022}
		}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to acknowledge Aleksandra Faust, Andy Christiansen, Chuyuan Fu, Daniel Kappler, David Rendleman, Eric Jang, Jessica Gomez, Jessica Lin, Jie Tan, Josh Weaver, Justin Boyd, Krzysztof Choromanski, Matthew Bennice, Mengyuan Yan, Mrinal Kalakrishnan, Nik Stewart, Paul Wohlhart, Peter Pastor, Pierre Sermanet, Wenlong Lu, Zhen Yu Song, Zhuo Xu, and the greater teams at Robotics at Google and Everyday Robots for their feedback and contributions.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
